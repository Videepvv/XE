{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91baff5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'crossEncoder (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from helper import tokenize, forward_ab, f1_score, accuracy, precision, recall\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from models import CrossEncoder\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_proposition_map(dataset):\n",
    "    data = f'./Data/{dataset}.csv'\n",
    "    df = pd.read_csv(data)\n",
    "    prop_dict = defaultdict(dict)\n",
    "    for x, y in enumerate(df.iterrows()):\n",
    "\n",
    "        prop_dict[x]['common_ground'] = df['Common Ground'][x]\n",
    "        prop_dict[x]['transcript'] = df['Transcript'][x]\n",
    "        prop_dict[x]['label'] = df['Label'][x]\n",
    "    return prop_dict, df\n",
    "\n",
    "\n",
    "def add_special_tokens(proposition_map):\n",
    "    for x, y in proposition_map.items():\n",
    "        #print(y['common_ground'])\n",
    "        cg_with_token = \"<m>\" + \" \" + y['common_ground']+ \" \"  + \"</m>\"\n",
    "        prop_with_token = \"<m>\" + \" \"+ y['transcript'] +\" \" + \"</m>\"\n",
    "        proposition_map[x]['common_ground'] = cg_with_token\n",
    "        proposition_map[x]['transcript'] = prop_with_token\n",
    "    return proposition_map\n",
    "\n",
    "def predict_with_XE(parallel_model, dev_ab, dev_ba, device, batch_size):\n",
    "    n = dev_ab['input_ids'].shape[0]\n",
    "    indices = list(range(n))\n",
    "    # new_batch_size = batching(n, batch_size, len(device_ids))\n",
    "    # batch_size = new_batch_size\n",
    "    all_scores_ab = []\n",
    "    all_scores_ba = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, n, batch_size), desc='Predicting'):\n",
    "            batch_indices = indices[i: i + batch_size]\n",
    "            scores_ab = forward_ab(parallel_model, dev_ab, device, batch_indices)\n",
    "            scores_ba = forward_ab(parallel_model, dev_ba, device, batch_indices)\n",
    "            all_scores_ab.append(scores_ab.detach().cpu())\n",
    "            all_scores_ba.append(scores_ba.detach().cpu())\n",
    "\n",
    "    return torch.cat(all_scores_ab), torch.cat(all_scores_ba)\n",
    "\n",
    "\n",
    "def train_prop_XE(dataset, model_name=None):\n",
    "    dataset_folder = f'./datasets/{dataset}/'\n",
    "    device = torch.device('cuda:0')\n",
    "    device_ids = list(range(1))\n",
    "    #load the statement and proposition data\n",
    "    prop_dict, df = make_proposition_map(\"Dataset\")\n",
    "    proposition_map = add_special_tokens(prop_dict)\n",
    "    \n",
    "    train_pairs  = [x for x in proposition_map.keys()]\n",
    "    train_labels = [y['label'] for x,y in proposition_map.items()]\n",
    "\n",
    "    dev_pairs = [x for x in proposition_map.keys()]\n",
    "    dev_labels = [y['label'] for x,y in proposition_map.items()]\n",
    "    \n",
    "    #try a sample unit test with a smaller pos/neg set\n",
    "    train_pairs =train_pairs[0:50] + train_pairs[-50:] # 50 pos and 50 neg labels\n",
    "    train_labels = train_labels[0:50] + train_labels[-50:] \n",
    "    dev_pairs = dev_pairs[0:50] + dev_pairs[-50:]  # 50 pos and 50 neg labels\n",
    "    dev_labels = dev_labels[0:50] + dev_labels[-50:]\n",
    "    \n",
    "    \n",
    "    model_name = 'roberta-base'\n",
    "    scorer_module = CrossEncoder(is_training=True,long=False,  model_name=model_name).to(device)\n",
    "\n",
    "    parallel_model = torch.nn.DataParallel(scorer_module, device_ids=device_ids)\n",
    "    parallel_model.module.to(device)\n",
    "    train(train_pairs, train_labels, train_pairs, train_labels, parallel_model, proposition_map, dataset_folder, device,\n",
    "          batch_size=4, n_iters=10, lr_lm=0.000001, lr_class=0.0001)\n",
    "  \n",
    "def tokenize_props(tokenizer, proposition_ids, proposition_map, m_end, max_sentence_len=1024, truncate=True):\n",
    "    if max_sentence_len is None:\n",
    "        max_sentence_len = tokenizer.model_max_length\n",
    "\n",
    "    pairwise_bert_instances_ab = []\n",
    "    pairwise_bert_instances_ba = []\n",
    "\n",
    "    doc_start = '<doc-s>'\n",
    "    doc_end = '</doc-s>'\n",
    "\n",
    "    for index in proposition_ids:\n",
    "        sentence_a = proposition_map[index]['transcript']\n",
    "        sentence_b = proposition_map[index]['common_ground']\n",
    "\n",
    "        def make_instance(sent_a, sent_b):\n",
    "            return ' '.join(['<g>', doc_start, sent_a, doc_end]), \\\n",
    "                   ' '.join([doc_start, sent_b, doc_end])\n",
    "\n",
    "        instance_ab = make_instance(sentence_a, sentence_b)\n",
    "        pairwise_bert_instances_ab.append(instance_ab)\n",
    "\n",
    "        instance_ba = make_instance(sentence_b, sentence_a)\n",
    "        pairwise_bert_instances_ba.append(instance_ba)\n",
    "\n",
    "    def truncate_with_mentions(input_ids):\n",
    "        input_ids_truncated = []\n",
    "        for input_id in input_ids:\n",
    "            m_end_index = input_id.index(m_end)\n",
    "\n",
    "            curr_start_index = max(0, m_end_index - (max_sentence_len // 4))\n",
    "\n",
    "            in_truncated = input_id[curr_start_index: m_end_index] + \\\n",
    "                           input_id[m_end_index: m_end_index + (max_sentence_len // 4)]\n",
    "            in_truncated = in_truncated + [tokenizer.pad_token_id] * (max_sentence_len // 2 - len(in_truncated))\n",
    "            input_ids_truncated.append(in_truncated)\n",
    "\n",
    "        return torch.LongTensor(input_ids_truncated)\n",
    "\n",
    "    def ab_tokenized(pair_wise_instances):\n",
    "        instances_a, instances_b = zip(*pair_wise_instances)\n",
    "\n",
    "        tokenized_a = tokenizer(list(instances_a), add_special_tokens=False)\n",
    "        tokenized_b = tokenizer(list(instances_b), add_special_tokens=False)\n",
    "\n",
    "        tokenized_a = truncate_with_mentions(tokenized_a['input_ids'])\n",
    "        positions_a = torch.arange(tokenized_a.shape[-1]).expand(tokenized_a.shape)\n",
    "        tokenized_b = truncate_with_mentions(tokenized_b['input_ids'])\n",
    "        positions_b = torch.arange(tokenized_b.shape[-1]).expand(tokenized_b.shape)\n",
    "\n",
    "        tokenized_ab_ = torch.hstack((tokenized_a, tokenized_b))\n",
    "        positions_ab = torch.hstack((positions_a, positions_b))\n",
    "\n",
    "        tokenized_ab_dict = {'input_ids': tokenized_ab_,\n",
    "                             'attention_mask': (tokenized_ab_ != tokenizer.pad_token_id),\n",
    "                             'position_ids': positions_ab\n",
    "                             }\n",
    "\n",
    "        return tokenized_ab_dict\n",
    "\n",
    "    if truncate:\n",
    "        tokenized_ab = ab_tokenized(pairwise_bert_instances_ab)\n",
    "        tokenized_ba = ab_tokenized(pairwise_bert_instances_ba)\n",
    "    else:\n",
    "        instances_ab = [' '.join(instance) for instance in pairwise_bert_instances_ab]\n",
    "        instances_ba = [' '.join(instance) for instance in pairwise_bert_instances_ba]\n",
    "        tokenized_ab = tokenizer(list(instances_ab), add_special_tokens=False, padding=True)\n",
    "\n",
    "        tokenized_ab_input_ids = torch.LongTensor(tokenized_ab['input_ids'])\n",
    "\n",
    "        tokenized_ab = {'input_ids': torch.LongTensor(tokenized_ab['input_ids']),\n",
    "                         'attention_mask': torch.LongTensor(tokenized_ab['attention_mask']),\n",
    "                         'position_ids': torch.arange(tokenized_ab_input_ids.shape[-1]).expand(tokenized_ab_input_ids.shape)}\n",
    "\n",
    "        tokenized_ba = tokenizer(list(instances_ba), add_special_tokens=False, padding=True)\n",
    "        tokenized_ba_input_ids = torch.LongTensor(tokenized_ba['input_ids'])\n",
    "        tokenized_ba = {'input_ids': torch.LongTensor(tokenized_ba['input_ids']),\n",
    "                        'attention_mask': torch.LongTensor(tokenized_ba['attention_mask']),\n",
    "                        'position_ids': torch.arange(tokenized_ba_input_ids.shape[-1]).expand(tokenized_ba_input_ids.shape)}\n",
    "\n",
    "    return tokenized_ab, tokenized_ba    \n",
    "    \n",
    "\n",
    "    \n",
    "def train(train_pairs,\n",
    "          train_labels,\n",
    "          dev_pairs,\n",
    "          dev_labels,\n",
    "          parallel_model,\n",
    "          proposition_map,\n",
    "          working_folder,\n",
    "          device,\n",
    "          batch_size=4,\n",
    "          n_iters=50,\n",
    "          lr_lm=0.00001,\n",
    "          lr_class=0.001):\n",
    "    bce_loss = torch.nn.BCELoss()\n",
    "    # mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': parallel_model.module.model.parameters(), 'lr': lr_lm},\n",
    "        {'params': parallel_model.module.linear.parameters(), 'lr': lr_class}\n",
    "    ])\n",
    "\n",
    "    # all_examples = load_easy_hard_data(trivial_non_trivial_path)\n",
    "    # train_pairs, dev_pairs, train_labels, dev_labels = split_data(all_examples, dev_ratio=dev_ratio)\n",
    "\n",
    "    tokenizer = parallel_model.module.tokenizer\n",
    "\n",
    "    # prepare data\n",
    "    train_ab, train_ba = tokenize_props(tokenizer, train_pairs, proposition_map, parallel_model.module.end_id, max_sentence_len=512)\n",
    "    dev_ab, dev_ba = tokenize_props(tokenizer, train_pairs, proposition_map, parallel_model.module.end_id, max_sentence_len=512)\n",
    "   \n",
    "    #labels\n",
    "    train_labels = torch.FloatTensor(train_labels)\n",
    "    dev_labels = torch.LongTensor(dev_labels)\n",
    "    print(\"train tensor size\",train_ab['input_ids'].size())\n",
    "    print(\"dev tensor size\",dev_ab['input_ids'].size())\n",
    "    print(\"train label size\", len(train_labels))\n",
    "    print(\"dev label size\", len(dev_labels))\n",
    "    train_loss = []\n",
    "    for n in range(n_iters):\n",
    "        break\n",
    "        train_indices = list(range(len(train_pairs)))\n",
    "        random.shuffle(train_indices)\n",
    "        iteration_loss = 0.\n",
    "        # new_batch_size = batching(len(train_indices), batch_size, len(device_ids))\n",
    "        new_batch_size = batch_size\n",
    "        for i in tqdm(range(0, len(train_indices), new_batch_size), desc='Training'):\n",
    "            optimizer.zero_grad()\n",
    "            batch_indices = train_indices[i: i + new_batch_size]\n",
    "\n",
    "            scores_ab = forward_ab(parallel_model, train_ab, device, batch_indices)\n",
    "            scores_ba = forward_ab(parallel_model, train_ba, device, batch_indices)\n",
    "\n",
    "            batch_labels = train_labels[batch_indices].reshape((-1, 1)).to(device)\n",
    "\n",
    "            scores_mean = (scores_ab + scores_ba) / 2\n",
    "\n",
    "            loss = bce_loss(scores_mean, batch_labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            iteration_loss += loss.item()\n",
    "\n",
    "        print(f'Iteration {n} Loss:', iteration_loss / len(train_pairs))\n",
    "        train_loss.append(iteration_loss / len(train_pairs))\n",
    "        # iteration accuracy\n",
    "        dev_scores_ab, dev_scores_ba = predict_with_XE(parallel_model, dev_ab, dev_ba, device, batch_size)\n",
    "        dev_predictions = (dev_scores_ab + dev_scores_ba)/2\n",
    "        dev_predictions = dev_predictions > 0.5\n",
    "        dev_predictions = torch.squeeze(dev_predictions)\n",
    "\n",
    "        print(\"dev accuracy:\", accuracy(dev_predictions, dev_labels))\n",
    "        print(\"dev precision:\", precision(dev_predictions, dev_labels))\n",
    "        print(\"dev recall:\", recall(dev_predictions, dev_labels))\n",
    "        print(\"dev f1:\", f1_score(dev_predictions, dev_labels))\n",
    "    plt.plot(train_loss)\n",
    "    plt.show()\n",
    "#         if n % 2 == 0:\n",
    "#             scorer_folder = working_folder + f'/XE_scorer/chk_{n}'\n",
    "#             if not os.path.exists(scorer_folder):\n",
    "#                 os.makedirs(scorer_folder)\n",
    "#             model_path = scorer_folder + '/linear.chkpt'\n",
    "#             torch.save(parallel_model.module.linear.state_dict(), model_path)\n",
    "#             parallel_model.module.model.save_pretrained(scorer_folder + '/bert')\n",
    "#             parallel_model.module.tokenizer.save_pretrained(scorer_folder + '/bert')\n",
    "#             print(f'saved model at {n}')\n",
    "\n",
    "#     scorer_folder = working_folder + '/XE_scorer/'\n",
    "#     if not os.path.exists(scorer_folder):\n",
    "#         os.makedirs(scorer_folder)\n",
    "#     model_path = scorer_folder + '/linear.chkpt'\n",
    "#     torch.save(parallel_model.module.linear.state_dict(), model_path)\n",
    "#     parallel_model.module.model.save_pretrained(scorer_folder + '/bert')\n",
    "#     parallel_model.module.tokenizer.save_pretrained(scorer_folder + '/bert')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_prop_XE('ecb', model_name='roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff14fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72cdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
